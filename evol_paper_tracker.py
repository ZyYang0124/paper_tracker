#!/usr/bin/env python3
# evol_paper_tracker.py
import requests
import datetime
import os
import json
import smtplib
import time
import xml.etree.ElementTree as ET
import argparse
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# ========== é»˜è®¤é…ç½® ==========
DEFAULT_JOURNALS = [
    "Nature", "Science", "Proc Natl Acad Sci U S A", "Cell",
    "Syst Biol", "Nat Ecol Evol", "Nat Genet", "Mol Biol Evol",
    "Cladistics", "Curr Biol"
]

DEFAULT_KEYWORDS = [
    "phylogen*", "systematic*", "evolution*", "genom*",
    '"phenotypic plasticity"', "adaptive radiation", "speciation",
    "molecular clock", "ancestral state reconstruction",
    "comparative genomics", "gene family evolution"
]

DASHSCOPE_API_KEY = "sk-b4f203c2f81341abb3e8ea34445f9f0f"  # â† æ›¿æ¢ä¸ºä½ è‡ªå·±çš„

EMAIL_CONFIG = {
    "smtp_server": "smtp.qq.com",
    "port": 465,
    "sender_email": "1214631670@qq.com",
    "password": "uktytxqmrccnidjb",
    "receiver_email": "yangzy0124@gmail.com"
}

BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
MAX_RETRIES = 3


import dashscope
from dashscope import Generation
dashscope.api_key = DASHSCOPE_API_KEY


def retry_on_fail(func, max_retries=MAX_RETRIES, delay_base=1):
    def wrapper(*args, **kwargs):
        for attempt in range(max_retries):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                wait = delay_base * (2 ** attempt)
                if attempt == max_retries - 1:
                    print(f"âŒ æœ€ç»ˆå¤±è´¥: {e}")
                    return None
                else:
                    print(f"âš ï¸ å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}ï¼Œ{wait}ç§’åé‡è¯•...")
                    time.sleep(wait)
        return None
    return wrapper


@retry_on_fail
def summarize_with_qwen(title, abstract):
    if not abstract.strip():
        return "æ— æ‘˜è¦ï¼Œæ— æ³•æ€»ç»“ã€‚"
    prompt = f"""ä½ æ˜¯ä¸€ä½è¿›åŒ–ç”Ÿç‰©å­¦ä¸“å®¶ï¼Œè¯·ç”¨ä¸€æ®µç®€æ´çš„ä¸­æ–‡ï¼ˆ100å­—ä»¥å†…ï¼‰æ€»ç»“ä»¥ä¸‹è®ºæ–‡çš„æ ¸å¿ƒå‘ç°ï¼š

æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{abstract[:1500]}
"""
    response = Generation.call(
        model="qwen-max",
        prompt=prompt,
        max_tokens=150,
        timeout=60
    )
    summary = response.output.text.strip()
    return summary if summary else "æ€»ç»“å¤±è´¥ã€‚"


def normalize_keyword(kw):
    kw = kw.strip()
    if kw.startswith('"') and kw.endswith('"'):
        kw = kw[1:-1]
    return kw


@retry_on_fail
def search_pubmed(journal, keywords, days=7):
    today = datetime.date.today()
    start_date = today - datetime.timedelta(days=days)

    kw_parts = [f'({normalize_keyword(kw)}[TIAB])' for kw in keywords if normalize_keyword(kw)]
    keyword_str = " OR ".join(kw_parts) if kw_parts else ""
    term = f'"{journal}"[Journal]'
    if keyword_str:
        term += f" AND ({keyword_str})"

    params = {
        "db": "pubmed",
        "term": term,
        "retmax": 200,
        "retmode": "json",
        "datetype": "pdat",
        "mindate": start_date.strftime("%Y/%m/%d"),
        "maxdate": today.strftime("%Y/%m/%d"),
    }

    print(f"\nğŸ” æ£€ç´¢æœŸåˆŠ: {journal}")
    r = requests.get(BASE_URL + "esearch.fcgi", params=params, timeout=20)
    r.raise_for_status()
    data = r.json()
    idlist = data.get("esearchresult", {}).get("idlist", [])
    print(f"âœ… æ‰¾åˆ° {len(idlist)} ç¯‡")
    return idlist


@retry_on_fail
def fetch_article(pmid):
    r = requests.get(BASE_URL + "efetch.fcgi", params={"db": "pubmed", "id": pmid, "retmode": "xml"}, timeout=20)
    r.raise_for_status()
    root = ET.fromstring(r.text)

    title_el = root.find(".//ArticleTitle")
    title = "".join(title_el.itertext()).strip() if title_el is not None else ""

    abstract_parts = []
    for ab in root.findall(".//Abstract/AbstractText"):
        label = ab.attrib.get("Label", "")
        text = "".join(ab.itertext()).strip()
        abstract_parts.append(f"{label}: {text}" if label else text)
    abstract = "\n".join([p for p in abstract_parts if p])

    journal_el = root.find(".//Journal/Title")
    journal = journal_el.text.strip() if journal_el is not None and journal_el.text else ""

    doi = next((aid.text.strip() for aid in root.findall(".//ArticleId")
                if aid.attrib.get("IdType", "").lower() == "doi" and aid.text), "")

    return {"pmid": pmid, "title": title, "abstract": abstract, "journal": journal, "doi": doi}


def load_processed(cache_file):
    if not os.path.exists(cache_file):
        return set()
    try:
        with open(cache_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            return set(data) if isinstance(data, list) else set()
    except:
        print("âš ï¸ ç¼“å­˜æ–‡ä»¶å¼‚å¸¸ï¼Œä»ç©ºé›†å¼€å§‹")
        return set()


def send_email(subject, body):
    msg = MIMEMultipart("alternative")
    msg["Subject"] = subject
    msg["From"] = EMAIL_CONFIG["sender_email"]
    msg["To"] = EMAIL_CONFIG["receiver_email"]
    msg.attach(MIMEText(body, "plain", "utf-8"))

    try:
        with smtplib.SMTP_SSL(EMAIL_CONFIG["smtp_server"], EMAIL_CONFIG["port"]) as server:
            server.login(EMAIL_CONFIG["sender_email"], EMAIL_CONFIG["password"])
            server.sendmail(EMAIL_CONFIG["sender_email"], EMAIL_CONFIG["receiver_email"], msg.as_string())
        print("ğŸ“§ é‚®ä»¶å‘é€æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
        raise


def parse_args():
    parser = argparse.ArgumentParser(
        description="ğŸ”¬ è¿›åŒ–ç”Ÿç‰©å­¦è®ºæ–‡è¿½è¸ªå™¨ï¼šä» PubMed æ£€ç´¢æŒ‡å®šæœŸåˆŠ/å…³é”®è¯çš„æ–°è®ºæ–‡ï¼Œç”¨ Qwen æ€»ç»“å¹¶é‚®ä»¶æ¨é€ã€‚",
        epilog="ç¤ºä¾‹ï¼špython evol_paper_tracker.py -j 'Nature,Science' -d 3 -n"
    )
    parser.add_argument("-j", "--journals", type=str,
                        help="æŒ‡å®šæœŸåˆŠåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼ˆå¦‚ï¼šNature,Scienceï¼‰")
    parser.add_argument("-k", "--keywords", type=str,
                        help="æŒ‡å®šå…³é”®è¯ï¼Œé€—å·åˆ†éš”ï¼ˆæ”¯æŒé€šé…ç¬¦ * å’Œå¼•å·ï¼‰")
    parser.add_argument("-d", "--days", type=int, default=7,
                        help="æ£€ç´¢æœ€è¿‘ N å¤©çš„è®ºæ–‡ï¼ˆé»˜è®¤ï¼š7ï¼‰")
    parser.add_argument("-n", "--no-email", action="store_true",
                        help="ä¸å‘é€é‚®ä»¶ï¼Œä»…ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶")
    parser.add_argument("-c", "--cache-file", type=str, default="processed_pmids.json",
                        help="å·²å¤„ç† PMID çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šprocessed_pmids.jsonï¼‰")
    parser.add_argument("-o", "--report-file", type=str,
                        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶åï¼ˆé»˜è®¤ï¼ševol_summary_YYYY-MM-DD.mdï¼‰")

    args = parser.parse_args()

    # è§£æ journals å’Œ keywords
    journals = [j.strip() for j in args.journals.split(",")] if args.journals else DEFAULT_JOURNALS
    keywords = [k.strip() for k in args.keywords.split(",")] if args.keywords else DEFAULT_KEYWORDS
    report_file = args.report_file or f"evol_summary_{datetime.date.today()}.md"

    return {
        "journals": journals,
        "keywords": keywords,
        "days": args.days,
        "no_email": args.no_email,
        "cache_file": args.cache_file,
        "report_file": report_file
    }


def main():
    config = parse_args()

    processed = load_processed(config["cache_file"])
    all_articles = {}
    new_pmids = []

    for journal in config["journals"]:
        pmids = search_pubmed(journal, config["keywords"], days=config["days"]) or []
        for pmid in pmids:
            if pmid in processed:
                continue
            art = fetch_article(pmid)
            if not art or not art["abstract"].strip():
                continue
            summary = summarize_with_qwen(art["title"], art["abstract"]) or "âš ï¸ æ€»ç»“å¤±è´¥"
            art["summary"] = summary
            j = art["journal"]
            if j not in all_articles:
                all_articles[j] = []
            all_articles[j].append(art)
            new_pmids.append(pmid)

    # ç”ŸæˆæŠ¥å‘Š
    lines = [f"# ğŸ§¬ è¿›åŒ–ç”Ÿç‰©å­¦æ¯æ—¥ç®€æŠ¥ ({datetime.date.today()})\n"]
    if not all_articles:
        lines.append("ä»Šæ—¥æ— ç›¸å…³æ–°è®ºæ–‡ã€‚")
    else:
        for journal, arts in all_articles.items():
            lines.append(f"## ğŸ“° {journal}\n")
            for art in arts:
                url = f"https://doi.org/{art['doi']}" if art['doi'] else f"https://pubmed.ncbi.nlm.nih.gov/{art['pmid']}/"
                lines.append(f"### [{art['title']}]({url})")
                lines.append(f"**AI æ€»ç»“**ï¼š{art['summary']}\n")
                lines.append(f"PMID: [{art['pmid']}](https://pubmed.ncbi.nlm.nih.gov/{art['pmid']}/)\n---\n")

    report = "\n".join(lines)
    with open(config["report_file"], "w", encoding="utf-8") as f:
        f.write(report)

    # æ›´æ–°ç¼“å­˜
    processed.update(new_pmids)
    with open(config["cache_file"], "w", encoding="utf-8") as f:
        json.dump(list(processed), f)

    # å‘é€é‚®ä»¶
    if new_pmids and not config["no_email"]:
        send_email(f"ã€è®ºæ–‡ç®€æŠ¥ã€‘{datetime.date.today()} - {len(new_pmids)} ç¯‡æ–°æ–‡ç« ", report)
    elif config["no_email"]:
        print("ğŸ“­ è·³è¿‡é‚®ä»¶å‘é€ï¼ˆ--no-email å¯ç”¨ï¼‰")
    else:
        print("ğŸ“­ ä»Šæ—¥æ— æ–°æ–‡ç« ï¼Œæœªå‘é€é‚®ä»¶ã€‚")


if __name__ == "__main__":
    main()